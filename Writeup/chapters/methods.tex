%!TEX root = ../thesis.tex
\begin{savequote}[75mm]
All life is an experiment.  The more experiments you make the better.
\qauthor{Ralph Waldo Emerson}
\end{savequote}

\chapter{Methods and Experiments}
\label{Methods}

\newthought{Throughout this project}, we work with the HeartSteps v1 Data, hereupon abbreviated HSv1.  Contextual features have been created from the measurements through domain science, through which we assume that the Bandit algorithms use linear models for the purposes of the project.

The contextual bandit algorithms used are stochastic, meaning for every user $n$, day $t$, and decision point $d$, the algorithm generates a probability $\pi_{n,t,d}$ of action.  Thus, a taken action is generated by

\begin{align*}
A_{n,t,d} \sim \operatorname{Bern}(\pi_{n,t,d}).
\end{align*}

The main metric of performance on a user we use is Mean User Expected Regret, or $MUER$. This is computed in equation \ref{MUER} using the reward $R^{(a)}_{n,t,d}$ of either action $a = 0$ or $a = 1$ under our generative model:
\begin{align}
\label{MUER}
MUER(\mathcal{S}, \Theta, \varepsilon,n) &= \frac{1}{TD} \sum_{\substack{t=1,\ldots,T \\ d=1,\ldots,D}} \left[\mathbb{E}_{\pi^\star} (R_{n,t,d}) - \mathbb{E}_{\pi_{n,t,d}} (R_{n,t,d}) \right],  \\
\label{GenReward}
R^{(a)}_{n,t,d} &= \begin{bmatrix} f_1(S_{n,t,d}) \\
a \cdot f_2(S_{n,t,d})
\end{bmatrix}^T \Theta + \epsilon_{n,t,d},  \\
\label{ExpectedReward}
\mathbb{E}_{\pi_{n,t,d}}\left(R_{n,t,d}\right) &:= \pi_{n,t,d} R^{(1)}_{n,t,d} + (1-\pi_{n,t,d}) R^{(0)}_{n,t,d} \\
\label{OptProb}
\pi^{\star}_{n,t,d} &:= \argmax_{\pi \in [\pi_{\text{min}},\pi_\text{max}]} \pi R_{n,t,d}^{(1)} - (1 -\pi) R_{n,t,d}^{(0)}
\end{align}

where expression \ref{GenReward} is the Reward of action $a \in [0,1]$ for user $n$ at time/decision point $t,d$, expression \ref{ExpectedReward}gives the expected reward given chosen probability $\pi_{n,t,d}$ of an activity suggestion action, and expression \ref{OptProb} chooses the optimal clipped probability $\pi \in [\pi_\text{min},\pi_\text{max}]$ knowing the rewards.

Note that we add in $\epsilon_{n,t,d}$ to better account for misspecification in our `true' generative model.


For each simulation of $N$ users, we can now compute the mean and standard deviation of $MUER$ as our main performance metrics. \\

\section{Methodology Overview}

For simulations, our overall methodology is the following.  We first designate a variant of the Bandit algorithm, as well as a replayer to set up our `true' generative model, using $f_1,f_2$, to form simulated rewards from the context and given action.  Next, we split HSv1 user data into $K$-fold cross validation sets.  For each split, we perform the below process:

\begin{enumerate}
	\item Use the training and testing data to generate training simulated users and testing simulated users, with $N = 2500$ users in simulation.  
	\item Use training simulated users to tune parameters of the given Bandit algorithm based on the mean of $MUER$ computed across all users and decision points, contingent on quality metrics.
	\item Run simulated users from test split using optimally tuned parameters, observing quality metrics and impact of each tuning parameter on the mean and standard deviation of all computed $MUER$.
\end{enumerate}

Each part is described in more detail in table \ref{Notation Table}.


\section{Reward Generative Model}

We set different `true' generative models, where we assume:
\begin{equation}
\label{True Generative Model}
\mathcal{R} = \begin{bmatrix}f_1(\mathcal{S}) \\
\mathcal{A} \odot f_2(\mathcal{S})
\end{bmatrix}^T \Theta + \varepsilon
\end{equation}

and that $f_1, f_2$ are feature functions from our set of contextual features $\mathcal{S}$ to baseline and interaction terms, $\odot$ denotes the term-wise product, and each $\varepsilon \sim \mathcal{N}\left(0, \sigma^2 \right)$ for $\sigma^2$, which is a tuning parameter with estimator $\hat{\sigma^2} = \operatorname{Var}(\varepsilon)$.  

We describe these models more in Section \ref{Models/Reward Generative Models}.





\section{Cross-Validation}

We use $K = 3$ fold cross-validation to separate training and testing batches.  We do not train the bandit algorithm's parameters on the test batches, as to simulate application of HSv1 data for HSv2.  

To do this, we randomly order the $N = 37$ users, then group the randomly ordered users into $K$ groups; the $K$-th fold cross-validation is performed holding the $K$-th group out as the test sample, and the remaining groups in as the training sample.  We then bootstrap from the given splits to form the requisite $N = 2500$ sample size.

For each user, we have a series across all days $T$ and decision points $t$ per day of Reward, Action, and Context.  We will refer to them jointly as $(R, A, S)_{(N,T,t)}$, which are implicitly indexed in order by the user, day, and decision point, creating $N \times T \times t$ data points.  



\section{Residual Formation}
\label{Residual Formation}

Within each test batch and train batch, we conduct ordinary least squares linear regression to residualize additional effects, missing from our model due to possible misspecification, for each user from the baseline and interaction effects.  Specifically, we use the model in Equation \ref{True Generative Model}.

Recall that $f_1$, $f_2$ are the baseline and interaction functions, that are parameters of the generative model.

In this way, we obtain a `true' $\Theta$ for the simulation, as well as a series of $\varepsilon$ corresponding with each data point.  This series will be used when computing the `true' generative model reward used in training and testing the Bandit model; note that this allows us to give the actual reward obtained in HSv1 when choosing the actual action used by the corresponding context from HSv1, but only getting an OLS estimate when choosing the other action.


\section{Simulated User Generation}
\label{Simulated User Generation}

For both the training and test original users' series of $(\mathcal{R}, \mathcal{A}, \mathcal{S})$, we can create train and test batches of $N = 2500$ users each by randomly sampling with replacement from the train and test pools respectively.  This value of $N$ was chosen to give each training user roughly $100$ runs and each testing user roughly $200$, as there are $24-25$ users in each training split and $12-13$ users in each testing split, as well as to give statistical significance in our stochastic algorithm.

We note finally that in each of the $N = 37$ original HSv1 users, we imputed the mean value for missing contextual feature measurements, and set availability to True when the reward and at least one of the contextual features is measured.



\section{Simulated User Testing and Quality Metrics}

Once the tuning parameters have been optimized for the batch, we run the test users on with these parameters, and analyze the quality metrics described below:

\begin{enumerate}
	\item Time series of cumulative expected regret
	\item Time series of $\pi_t(1 | S_t)$, the probability of taking action $1$
	\item Histogram of $|\pi_t(1 | S_t) - opt_t(S_t)|$
	\subitem We define $opt_t(S_t)$ as the optimal probability in context $S_t$:
	\begin{equation}
	\label{opt equation}
		opt_t(S_t) = 0.8\ \mathbbm{1}\{\text{optimal action is $1$ in $S_t$}\} + 0.2\ \mathbbm{1}\{\text{optimal action is $0$ in $S_t$}\} .
	\end{equation}
	\item Histogram of number of actions taken per day
	\item Time series of number of actions taken per day
	\item Histogram of number of times per day that the feedback controller was invoked
	\item Tim series of number of times per day that the feedback controller was invoked
	\item Time series of MSE between `True' $\Theta$ and the Bandit's estimate $\mu_\Theta$
\end{enumerate}


