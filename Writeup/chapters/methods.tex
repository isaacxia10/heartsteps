%!TEX root = ../thesis.tex
\begin{savequote}[75mm]
All life is an experiment.  The more experiments you make the better.
\qauthor{Ralph Waldo Emerson}
\end{savequote}

\chapter{Methods and Experiments}
\label{Methods}

\newthought{Throughout this project}, we work with the HeartSteps v1 Data, hereupon abbreviated HSv1.  Contextual features have been created from the measurements through domain science, and we assume that the Bandit algorithms use linear models for the purposes of the project.

The contextual bandit algorithms used are stochastic, meaning for every user $n$, day $t$, and decision point $d$, the algorithm generates a probability $\pi_{n,t,d}$ of action.  Thus, a taken action is generated by

\begin{align*}
A_{n,t,d} \sim \operatorname{Bern}(\pi_{n,t,d}).
\end{align*}

The main metric of performance on a user we use is Mean User Expected Regret, or $MUER$. This is computed in equation \ref{MUER} using the reward $R^{(a)}_{n,t,d}$ of either action $a = 0$ or $a = 1$ under our generative model:
\begin{align}
\label{MUER}
MUER(\mathcal{S}, \Theta, \varepsilon,n) &= \frac{1}{TD} \sum_{\substack{t=1,\ldots,T \\ d=1,\ldots,D}} \left[\mathbb{E}_{\pi^\star} (R_{n,t,d}) - \mathbb{E}_{\pi_{n,t,d}} (R_{n,t,d}) \right],  \\
\label{GenReward}
R^{(a)}_{n,t,d} &= \begin{bmatrix} f_1(S_{n,t,d}) \\
a \cdot f_2(S_{n,t,d})
\end{bmatrix}^T \Theta + \epsilon_{n,t,d},  \\
\label{ExpectedReward}
\mathbb{E}_{\pi_{n,t,d}}\left(R_{n,t,d}\right) &:= \pi_{n,t,d} R^{(1)}_{n,t,d} + (1-\pi_{n,t,d}) R^{(0)}_{n,t,d} \\
\label{OptProb}
\pi^{\star}_{n,t,d} &:= \argmax_{\pi \in [\pi_{\text{min}},\pi_\text{max}]} \pi R_{n,t,d}^{(1)} - (1 -\pi) R_{n,t,d}^{(0)}
\end{align}

where expression \ref{GenReward} is the Reward of action $a \in [0,1]$ for user $n$ at time/decision point $t,d$, expression \ref{ExpectedReward}gives the expected reward given chosen probability $\pi_{n,t,d}$ of an activity suggestion action, and expression \ref{OptProb} chooses the optimal clipped probability $\pi \in [\pi_\text{min},\pi_\text{max}]$ knowing the rewards.

Note that we add in $\epsilon_{n,t,d}$ to better account for misspecification in our `true' generative model.


For each simulation of $N$ users, we can now compute the mean and standard deviation of $MUER$ as our main performance metrics. \\

\section{Methodology Overview}

For simulations, our overall methodology is the following.  We first designate a variant of the Bandit algorithm, as well as a replayer to set up our `true' generative model, using $f_1,f_2$, to form simulated rewards from the context and given action.  Next, we split HSv1 user data into $K$-fold cross validation sets.  For each split, we perform the below process:

\begin{enumerate}
	\item Use the training and testing data to generate training simulated users and testing simulated users, with $N = 2500$ users in simulation.  
	\item Use training simulated users to tune parameters of the given Bandit algorithm based on the mean of $MUER$ computed across all users and decision points, contingent on quality metrics.
	\item Run simulated users from test split using optimally tuned parameters, observing quality metrics and impact of each tuning parameter on the mean and standard deviation of all computed $MUER$.
\end{enumerate}

Each part is described in more detail in table \ref{Notation Table}.


\section{Reward Generative Model}

We set different `true' generative models, where we assume:
\begin{equation}
\label{True Generative Model}
\mathcal{R} = \begin{bmatrix}f_1(\mathcal{S}) \\
\mathcal{A} \odot f_2(\mathcal{S})
\end{bmatrix}^T \Theta + \varepsilon
\end{equation}

and that $f_1, f_2$ are feature functions from our set of contextual features $\mathcal{S}$ to baseline and interaction terms, $\odot$ denotes the term-wise product, and each $\varepsilon \sim \mathcal{N}\left(0, \sigma^2 \right)$ for $\sigma^2$, which is a tuning parameter with estimator $\hat{\sigma^2} = \operatorname{Var}(\varepsilon)$.  

We describe these models more in Section \ref{Models/Reward Generative Models}.

\section{Data Treatment}

We describe the process of our treatment of HS v1 data here, loaded in from \verb+suggest-analysis-kristjan.csv+.

	\subsection{Dropped Data Points}

	Upon loading the data in \verb+suggest-analysis-kristjan.csv+, we only consider data points in which the HS user was sent either a physical activity suggestion message or no message at all; that is, we drop all times in which the user was sent a sedentary suggestion message.  We also drop all data points in which there were zero features that were not null; in other words, we keep a data point if there is at least one feature that is not missing, to be imputed later; the most salient example is the ``Standard deviation of step count in last 7 day'' feature, where there was no such standard deviation for the first $7$ days.

	\subsection{Data Standardization and Data Imputation}

	After dropping the previously described data points, we standardize each of the features to be mean $0$ and standard deviation $1$, pooling across all remaining data points for all users.  Missing data (null values) was not used in this standardization.

	After the standardization has been computed, we then mean impute any missing data to have the mean of the feature, which was $0$; note that this reduces the standard deviation of the feature based on how much data was missing.

	The ``Temperature'' feature occassionally gave a reading of $-1024$ degrees when there was an error in measurements; any such occurrence was replaced with a null value.

	The reward, the proximal step count, is never standardized or imputed.



\section{Cross-Validation}

We use $K = 3$ fold cross-validation to separate training and testing batches.  We do not train the bandit algorithm's parameters on the test batches, as to simulate application of HSv1 data for HSv2.  

To do this, we randomly order the $N = 37$ users, then group the randomly ordered users into $K$ groups; the $K$-th fold cross-validation is performed holding the $K$-th group out as the test sample, and the remaining groups in as the training sample.  We then bootstrap from the given splits to form the requisite $N = 2500$ sample size.

For each user, we have a series across all days $T$ and decision points $t$ per day of Reward, Action, and Context.  We will refer to them jointly as $(R, A, S)_{(N,T,t)}$, which are implicitly indexed in order by the user, day, and decision point, creating $N \times T \times t$ data points.  


\section{Residual Formation}
\label{Residual Formation}

Within each test batch and train batch, we conduct ordinary least squares linear regression to residualize additional effects, missing from our model due to possible misspecification, for each user from the baseline and interaction effects.  Specifically, we use the model in Equation \ref{True Generative Model}.

Recall that $f_1$, $f_2$ are the baseline and interaction functions, that are parameters of the generative model.

In this way, we obtain a `true' $\Theta$ for the simulation, as well as a series of $\varepsilon$ corresponding with each data point.  This series will be used when computing the `true' generative model reward used in training and testing the Bandit model; note that this allows us to give the actual reward obtained in HSv1 when choosing the actual action used by the corresponding context from HSv1, but only getting an OLS estimate when choosing the other action.


\section{Simulated User Generation}
\label{Simulated User Generation}

For both the training and test original users' series of $(\mathcal{R}, \mathcal{A}, \mathcal{S})$, we can create train and test batches of $N = 2500$ users each by randomly sampling with replacement from the train and test pools respectively.  This value of $N$ was chosen to give each training user roughly $100$ runs and each testing user roughly $200$, as there are $24-25$ users in each training split and $12-13$ users in each testing split, as well as to give statistical significance in our stochastic algorithm.

We note finally that in each of the $N = 37$ original HSv1 users, we imputed the mean value for missing contextual feature measurements, and set availability to True when the reward and at least one of the contextual features is measured.



\section{Simulated User Testing and Quality Metrics}

Once the tuning parameters have been optimized for the batch, we run the test users on with these parameters, and analyze the quality metrics described in table \ref{QM Table}.

\begin{table}[H]
\centering
\begin{tabular}{rr|p{10cm}}
\toprule
QM &  Type &  Description \\ 
\midrule
1 & Regret & Time series of cumulative expected regret averaged over all $t$ for all users as well as for all users \\
2 & & Time series of cumulative expected regret, with mean and various quantiles \\
\hline
3 & Actual Regret & Time series of cumulative actual regret (using the Bandit's actual chosen action) averaged over all $t$ for all users as well as for all users \\
4 &  & Time series of cumulative actual regret, with mean and various quantiles \\
\hline
5 & Probability & Time series of probs $\pi_t(1 | S_t)$ for all $t$, with mean and various quantiles \\
18 & & Time series of proportion of users with Probabilities that can be clipped ($\pi_t(1 | S_t) \not\in [\pi_{min},\pi_{max}]$) \\
19 & & Time series of distance of probs $\pi_t(1 | S_t)$ from convergence to a clipped value: $\displaystyle \min \left(| \pi_t(1 | S_t) - \pi_{min}|, |\pi_{max} - \pi_t(1 | S_t)| \right)$ \\
\hline
6 & Prob. vs Opt. & Time series of action probabilities versus the optimal action: $|\pi_t(1 | S_t) - opt_t(S_t)|$, over all $t$ for all users (see \ref{opt equation}) \\
7 & & Histogram for all users' $|\pi_t(1 | S_t) - opt_t(S_t)|$ averaged across all decision time points $t$ \\
\hline
8 & Action & Histogram of the number of actions (physical suggestion sent) per day for each user and each day \\
9 & & Time series of the number of actions per day taken across all users for all $t$ \\
\hline
10 & FC & Histogram of nuber of times per day that the feedback controller was invoked, taken across all users and decision time points \\
11 &  & Time series of proportion of simulation users that invoked the feedback controller \\
12 &  & Time series of number of times per day that the feedback controller was invoked across all users, with mean and various quantiles \\
\hline
13 & Theta MSE & Time series of MSE between ``True'' generative model's $\Theta$ and the Bandit's $\hat{\Theta}$ \\
14 & & Time series of MSE between ``True'' and Bandit $\Theta$, with mean and various quantiles \\
\hline
15 & Treatment Effect & Histogram and KDE of the effect of the treatment $\Theta_2^T f_2(S)$, broken up by week \\
16 & & Time series of the treatment effect, with mean and various quantiles \\
17 & & Scatter plots of prob $\pi_t(1 | S_t)$ against the treatment effect $\Theta_2^T f_2(S)$, broken up by week \\
\bottomrule
\end{tabular}
\caption{Quality Metric Plot Descriptions}
\label{QM Table}
\end{table}

We also use the expression $opt_t(S_t)$ to represent the optimal action in the set $\{0,1\}$ based on the context at time $t$, denoted below:

\begin{equation}
\label{opt equation}
	opt_t(S_t) = 0.8\ \mathbbm{1}\{\text{optimal action is $1$ in $S_t$}\} + 0.2\ \mathbbm{1}\{\text{optimal action is $0$ in $S_t$}\} .
\end{equation}
