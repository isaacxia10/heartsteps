%!TEX root = ../thesis.tex
\begin{savequote}[75mm]
This modeling thing, it's pretty easy, but actually it's also really tough.
\qauthor{Cara Delevigne}
\end{savequote}

\chapter{Models and Algorithms}
\label{Models}

\newthought{Several different variants} of Thompson Sampling applied to the multi-armed contextual bandit problem were investigated to test feasibility in the HeartSteps mobile application for future iterations.  Each variant tested the inclusion or exclusion of a set of features of the bandit algorithm, as detailed in this section.


\section{Replayer: True Reward Model}
\label{Models/Reward Generative Models}

We require setting a `True reward model in order to run and evaluate the Bandit algorithm variants.  However, we do not have such a reward model from HeartSteps v1 data, because we only have the observed reward associated with one of the possible actions taken at every time point.  As such, we create a replayer as in \citet{Li2011}, which is an unbiased estimator that we can use for offline evaluation, and furthermore can be used to compare different Bandit variants by keeping the same true generative model throughout; future work may investigate whether the same findings hold under different generative models.

To generate the `True' rewards from contexts, we set the `True' generative as the full set of features, and only feed in contexts from true data.  As we feed in real contexts, we can utilize the actual rewards to form estimators of generated rewards for actions that were not selected; this is explored further in sections \ref{Residual Formation} and \ref{Simulated User Generation}.

All contextual features are defined in Table \ref{Features Table}.  


\section{Bandit Reward Models}
Two types of reward models were specified for use in the Bandit algorithms: the {\it Full model} and the {\it Small model}. 
\label{Small Set Features} The Full model uses the all of the engineered features, while the Small model uses smaller set of features to train over, to determine efficacy of including additional features in the Bandit's generative model.  Features in the Full model were previously selected based on domain science and statistical significance, while Features in the Small model were selected by Backward Stepwise Regression from the Full model, removing features until all were significant in the regression.

In most of the tests, we specify the Bandit's reward model to be the same as the `True' reward model, sans the residual reward from OLS that we add back in that emulates the misspecified portion of the `True' reward model.  However, in some tests, we emulate further misspecification by removing additional features of the context and using the {\it Small model} for the Bandit's reward model.

\begin{table}[h!]
 \caption{Contextual Features in HeartSteps}
 \label{Features Table}	
 \centering \begin{tabular*}
{0.987\textwidth}
{|p{0.16\textwidth}|p{0.43\textwidth}|p{0.082\textwidth}|p{0.082\textwidth}|p{0.09\textwidth}|}
\toprule
Feature & Description & Interact & Small & Small Interact \\
\midrule
Study Day & Participant's day in study, with gaps in time removed & \vfil\hfil \checkmark &  & \vfil\hfil \checkmark \\
\hline
Work location indicator & Binary indicator for location at user's work or not &  & \vfil\hfil \checkmark &  \\
\hline
Other location indicator & Binary indicator for location not at user's work nor home & \vfil\hfil \checkmark &  & \vfil\hfil \checkmark \\
\hline
Standard deviation of step count in last 7 days & Computed as standard deviation of total step counts in hour-long interval\newline $[\text{decision point time} - 30 \text{ mins},\newline  \text{decision point time} + 30 \text{ mins}]$ from each of the last $7$ days & \vfil\hfil \checkmark & \vfil\hfil \checkmark &  \\
\hline
Step count in previous 30 minutes &  Computed as $\log(\text{Tracker}$ step count in 30 minutes before decision $\text{point}+ 0.5)$  &  & \vfil\hfil \checkmark &   \\
\hline
Square-root steps yesterday & Computed as $\sqrt{\text{Total tracked steps from previous day}}$ &  & \vfil\hfil \checkmark &   \\
\hline
Temperature & Outdoor temperature at time of decision point &  &  &  \\
\midrule 
\midrule
{\it Reward:} Step count in 30 minutes following decision point & Computed as $\log(\text{Tracker}$ step count in 30 minutes after decision $\text{point} + 0.5)$  & & & 
\\
\bottomrule
\end{tabular*}
  \end{table}

\section{Bandit Algorithm Features}
\label{Models/Bandit Algorithm Features}

In this section, we describe features non-standard to Linear Contextual Thomson Sampling that are used in all of our Bandit algorithms.

\subsection{Batch Model Update: Daily Model Updates}

While standard bandit models update their internal models after each action-reward observation, we opt to update our models only at the end of the day, batch updating $5$ decision time points together.  This yields several benefits, the foremost of which is to protect against overfitting -- updating less often buffers against the noisy contextual data, as well as giving the model a holistic view into the user's behavior over an entire day rather than for different hours throughout a day.  A secondary benefit is that this allows the HeartSteps mobile application to only contact the cloud for statistical learning only once a day, which can be valuable for conserving battery-life.

We opt to always include this feature in our variants of the Bandit, but in future work it may be valuable to consider varying the amount of time between each update of the Bandit's generative models.

\subsection{Time Varying Reward Function: Gaussian Process Prior}

In the standard multi-armed bandit setting, we must make the assumption that the reward function $r(S_t,A_t)$ does not change over time.  However, in the HeartSteps setting, it is not clear that the reward function does not change, as users' preferences for activity change as they progress through the duration of the study, and furthermore there may be additional unobserved contexts.  To model this change, we allow for a time-varying reward function by setting our prior to be a Gaussian process, which allows baseline reward to be IID and time invariant under the assumption that the reward coefficients sequence $\Theta_{(t,d)}$ are marginally normal according to the initialized prior parameters \citep{Greenewald2017}.  In addition, a secondary benefit of using a non-stationary reward model is that this forces the Bandit algorithm to continue exploration, as the Bandit algorithm is forced to adapt to changing environments; these updates are maintained on line \ref{Gaussian Process Update Procedure in Bandit} of algorithm \ref{HeartSteps Full Bandit Algorithm}.  \\


The Gaussian Process Prior has an update rate parameter $\gamma$ and prior parameters $\displaystyle \left( \mu_{\Theta}, \Sigma_\Theta \right)$, and is a time-varying set of model coefficients of $\{\Theta_{(t,d)}\}_{(t,d)} \le (T, D)$ over the course of the study. \\

For a single user, our reward model is 
\begin{align}
	\displaystyle R_{(t,d)+1}  &= \begin{bmatrix} f_1(S_{(t,d)}) \\
A_{(t,d)} \cdot f_2(S_{(t,d)})
\end{bmatrix}^T \Theta_{(t,d)} + \epsilon_{(t,d)} = F_{(t,d)}^T \Theta_{(t,d)} + \epsilon_{(t,d)}, \label{Reward Linear Model} \\
\epsilon_{(t,d)} &\stackrel{iid}{\sim} \mathcal{N}\left(0, \sigma^2\right).
\end{align}

Note that we assume iid reward residuals, and that the reward from context/action $S_{(t,d)}, A_{(t,d)}$ at time $(t,d)$ yields a reward $R_{(t,d)+1}$ measured at the `next' time.

Denote a history of contexts, actions, and rewards at time $(t,d)$ as all information gathered before that time; that is,
\begin{align}
  	\mathcal{H}_{(t,d)} := \left\{S_{(t',d')}, A_{(t',d')}, R_{(t',d') + 1} \right\}_{(t',d') < (t,d)}.
  \end{align}  Then, our Gaussian Prior updates knowing the history are are defined by 

\begin{align}
	\Theta_{(t,d)} | H_{(t,d)} &\sim \mathcal{N}\left(\mu_{(t,d)}, \Sigma_{(t,d)}\right); \\
	\mu_{(t,d)} &= \gamma \mu_{(t,d)-1} + (1-\gamma) \mu_{\Theta}, \label{GP Mu Update}\\
	\Sigma_{(t,d)} &= \gamma^2 \Sigma_{(t,d)-1} + (1-\gamma^2) \Sigma_\Theta.
\end{align}

To derive the posterior, we find the joint distribution of $\begin{bmatrix}\Theta_{(t,d)} \\
R_{(t,d)+1}
\end{bmatrix} | \left\{\mathcal{H}_{(t,d)}, S_{(t,d)}, A_{(t,d)} \right\}$, which is Gaussian itself. \\

 Recalling our model from \ref{Reward Linear Model} for the reward $R_{(t,d)+1}$ and that $S_{(t,d)} | \mathcal{H}_{(t,d)} \indep A_{(t,d)} | S_{(t,d)}, \mathcal{H}_{(t,d)}$, we have that
$$\mathrm{Cov}\left(\Theta_{(t,d)}, R_{(t,d)+1}\right) = \Cov\left(F_{(t,d)}^T \Theta_{(t,d)} + \epsilon_{(t,d)};\  \Theta_{(t,d)} \right) = F_{(t,d)}^T \Sigma_{(t,d)},$$ so we obtain the joint distribution:

\begin{align}
	\begin{bmatrix} \Theta_{(t,d)} \\
R_{(t,d)+1}
\end{bmatrix} | \left\{\mathcal{H}_{(t,d)}, S_{(t,d)}, A_{(t,d)} \right\} &\sim \mathcal{N} \left( \begin{bmatrix} \mu_{(t,d)} \\
F_{(t,d)}^T \mu_{(t,d)}
\end{bmatrix}, \begin{bmatrix} \Sigma_{(t,d)} & \Sigma_{(t,d)} F_{(t,d)} \\
F_{(t,d)}^T \Sigma_{(t,d)} & F_{(t,d)}^T \Sigma_{(t,d)} F_{(t,d)} + \sigma^2
\end{bmatrix}\right).
\end{align}

Combining the result giving conditionals within the Gaussian with the Sherman-Morrison inversion formula, we obtain that the posterior on the additional history of \\
\noindent $H_{(t,d)+1} = \left\{H_{(t,d)}, S_{(t,d)}, A_{(t,d)}, R_{(t,d)+1} \right\}$ is
\begin{align*}
	\Theta_{(t,d)} | H_{(t,d)+1} &\sim \mathcal{N}\left(\hat{\mu}_{(t,d)}, \hat{\Sigma}_{(t,d)}\right), \text{ where}: \\
	\hat{\mu}_{(t,d)} &= \mu_{(t,d)} + \left(\Sigma_{(t,d)} F_{(t,d)}\right) \left(F_{(t,d)}^T \Sigma_{(t,d)} F_{(t,d)} + \sigma^2  \right)^{-1} (R_{(t,d)+1} - F_{(t,d)}^T \mu_{(t,d)}) \\
	&= \mu_{(t,d)} + \left(\frac{1}{\sigma^2 + F_{(t,d)}^T \Sigma_{(t,d)} F_{(t,d)}} \right)\Sigma_{(t,d)} F_{(t,d)} (R_{(t,d+1)} - F_{(t,d)}^T \mu_{(t,d)}), \\
	\hat{\Sigma}_{(t,d)} &= \Sigma_{(t,d)} - \Sigma_{(t,d)} F_{(t,d)} \left(F_{(t,d)}^T \Sigma_{(t,d)} F_{(t,d)}\right)^{-1} F_{(t,d)}^T \Sigma_{(t,d)}\\
	&= \Sigma_{(t,d)} - \left(\frac{1}{\sigma^2 + F_{(t,d)}^T \Sigma_{(t,d)} F_{(t,d)}} \right) \Sigma_{(t,d)} F_{(t,d)} F_{(t,d)}^T \Sigma_{(t,d)}.
\end{align*}



\section{Thompson Sampling Variants}
\label{Models/Bandit Algorithm Variants}

Now, we describe non-standard features used in the variants of the Thompson Sampling algorithm tested.  While each feature address specific problems in HeartSteps, we test how including features impact the performance of the Bandit, and whether it makes sense to exclude for the sake of parsimony.

\subsection{Action Centering}

A highly salient observation is that in order to choose the optimal action, our Bandit algorithm does not need to predict the entirety of the reward function, but just be able to estimate the interaction portion of the reward function.  Specifically, rewriting our reward model as $R_{(t,d)_1} = f_1(S_{(t,d)})^T \Theta_{1:p_1 + 1} + A_{(t,d)} f_2(S_{(t,d)})^T \Theta_{p_1+1 : p_1+p_2} + \varepsilon_{(t,d)}$, we are only concerned with the term where the action $A_{(t,d)}$ can affect the reward, and thus with the sub-parameter $\Theta_{p_1+1 : p_1+p_2}$.

One way to isolate this effect is to centering the action as in \citet{Greenewald2017}.  Here, instead of setting the Bandit to directly use the step-count rewards of $R_{(t,d)+1)}$, we use the differential reward of $R_{(t,d)+1} - R_{(t,d)+1}^{(0)}$, where $R_{(t,d)+1}^{(0)}$ is the reward of taking action $0$ at time $(t,d)$.  Then, we see that
\begin{align}
	R_{(t,d)+1} - R_{(t,d)+1}^{(0)} &= A_{(t,d)} f_2(S_{(t,d)})^T \Theta_{p_1+1 : p_1+p_2} + \varepsilon_{(t,d)}',
\end{align}

for sub-Gaussian noise $\varepsilon_{(t,d)}'$ with variance $\sigma^2$.


Applying this feature allows the Bandit to learn a less complex reward model, while still preserving the same reward benefits as learning the full reward model.

To apply this feature, we simply use the unbiased estimator $(A_{(t,d)} - \pi_{(t,d)}) R_{(t,d)+1}$ for $R_{(t,d)+1} - R_{(t,d)+1}^{(0)}$.  This appears on line \ref{Action Centering in Bandit} of algorithm \ref{HeartSteps Full Bandit Algorithm}; to remove the use of this feature, we change the line to $m_t \leftarrow [f_1(S_t), A_t f_2(S_t)]$.


\subsection{Feedback Controller}

A challenge in mHealth is to avoid user disengagement.  To address this concern, the Bandit algorithm alleviates the problem of sending too many or too few suggestions by controlling $\pi_{(t,d)}$, the probability of selecting an active suggestion action $A_{(t,d)} = 1$, based on the recent dosage, which is measured as the count of active suggestion actions given to the user.  If there have been too many active suggestion actions in the last few decision points, the Bandit algorithm decreases the probability of selecting an active suggestion relative to its original proposed probability.  A secondary benefit is that this better allows for postmortem inference, as the researcher will have access to the result of selecting non-locally optimal actions.

Specifically, our feedback controller consists of parameters $(\lambda, N_c, T_c)$.  In step \ref{Feedback Controller in Bandit} of Algorithm \ref{HeartSteps Full Bandit Algorithm}, the feedback controller simply makes it harder to select a probability of an making an active suggestion through the limiting term $\lambda (N_t - N_c)_+$; removing the feedback controller replaces this term with $0$.

\subsection{Probability Clipping}

To ensure that the bandit does not suffer from a poor initialization stay in a local maximum, we clip all action probabilities $\pi \in [\pi_{\text{min}}, \pi_\text{max}]$ to be within a certain probability range.  For the experiment, we set $[\pi_\text{min},\pi_\text{max}] = [0.1,0.8]$; this allows the Bandit algorithm enough slack to sometimes choose locally suboptimal actions to further exploration of the reward function.  An additional benefit is that this limits the maximum expected number of activity suggestion actions while allowing the algorithm a reasonable probability range to adapt to large changes in contexts.  Without this feature, our algorithm changes in step \ref{Probability Clipping in Bandit} of algorithm \ref{HeartSteps Full Bandit Algorithm}, instead becoming $\pi_t \leftarrow \mathbb{P}\left[f_2(S_t)^T X > 0 \right]$.

\clearpage


\begin{algorithm}[!hp]%
\setstretch{0.75}
\thisfloatpagestyle{empty}

 \KwData{\begin{enumerate}[noitemsep]
 	\item Gaussian Process Prior $(\gamma, \mu_\Theta, \Sigma_\Theta)$
 	\item Reward noise estimate $\sigma^2$
 	\item Feedback Controller parameters $(\lambda, N_c, T_c)$
 	\item Probability Clipping parameters $(\pi_{\text{min}}, \pi_{\text{max}})$
 	\item Bandit Reward model mappings: Baseline mapping $f_1: \mathcal{S} \to \mathbb{R}^{p_1}$, Interaction mapping $f_2: \mathcal{S} \to \mathbb{R}^{p_2}$.
 \end{enumerate}}
 \SetKwInput{KwVars}{Posterior Parameters}
 \KwVars{\begin{enumerate}[noitemsep]
 	\item $(\mu_{\text{start}}, \Sigma_{\text{start}})$: Start-of-day Bandit reward model posterior
 	\item $(\mu_{\text{cur}}, \Sigma_{\text{cur}})$: Bandit reward model posterior, updated throughout decision points
 \end{enumerate}}

 $\mu_{\text{start}},\Sigma_{\text{start}} \leftarrow \mu_\Theta, \Sigma_\Theta$ \tcp*{Set start-of-day posterior} 
 \For{$1 \le t \le T = 90$}{
 	$\mu_{\text{cur}}, \Sigma_{\text{cur}} \leftarrow \mu_{\text{start}}, \Sigma_{\text{start}}$ \tcp*{Set current posterior}
 	\For{$1 \le d \le 5$\tcp*{Iterate through decision points}} {

 		Obtain $S_{(t,d)}$, current context vector\;
 		Compute $N_{(t,d)}$, the dosage from past $T_c$ decision times\;
 		$X \sim \mathcal{N}_{p_2} \left(\mu_\text{cur}[p_1:p_1+p_2], \Sigma_\text{cur}[p_1:p_1+p_2,p_1:p_1+p_2]\right)$ \tcp*{Randomly sample from Gaussian posterior distribution of interaction term only}
 		$\displaystyle \pi_{(t,d)} \leftarrow \mathbb{P} \left[f_2(S_t)^T X > \lambda (N_t - N_c)_+ \right]$ \tcp*{Use Feedback Controller to compute unclipped randomization probability} \label{Feedback Controller in Bandit}

 		$\pi_t \leftarrow \min\left(\pi_\text{max}, \max(\pi_{(t,d)}, \pi_{\text{min}}) \right)$ \tcp*{Probability Clipping} \label{Probability Clipping in Bandit}

 		Return action $A_{(t,d)} \sim \text{Bern}(\pi_{(t,d)})$, collect reward $R_{(t,d)+1}$ \tcp*{Bandit action and reward observation}

		$\mu_{\text{cur}} \leftarrow (1-\gamma)\mu_\Theta + \gamma \mu_{\text{start}}, \ \ \Sigma_{\text{cur}} \leftarrow (1-\gamma^2) \Sigma_\Theta + \gamma^2 \Sigma_\text{start}$ \tcp*{Update current parameters as posterior of next decision time point}
 	}

 	$\mu_{\text{cur}}, \Sigma_{\text{cur}} \leftarrow \mu_{\text{start}}, \Sigma_{\text{start}}$ \tcp*{Reset current posterior to start-of-day posterior}
 	\tcp{Perform end-of-day batch update to Bandit Models}
 	\For{$1 \le d \le 5$} {
 		$F_{(t,d)} \leftarrow \left[f_1(S_{(t,d)}), (A_{(t,d)} - \pi_{(t,d)})f_2(S_{(t,d)}) \right]$ \tcp{Full model feature vector, Action-Centering}
 		\tcp{Gaussian Process Update Procedure} \label{Action Centering in Bandit}
 		\begin{align*}
 			\hat{\mu}_\text{cur} &\leftarrow \mu_\text{cur} + \left(\frac{R_{(t,d)+1} - F_{(t,d)}^T \mu_\text{cur}}{\sigma^2 + F_{(t,d)}^T\Sigma_\text{cur}F_{(t,d)}}\right)\Sigma_\text{cur}F_{(t,d)} ;\\
 			\hat{\Sigma}_\text{cur} &\leftarrow \Sigma_\text{cur} - \left(\frac{1}{\sigma^2 +F_{(t,d)}^T \Sigma_\text{cur} F_{(t,d)}}\right)\Sigma_\text{cur} F_{(t,d)} F_{(t,d)}^T \Sigma_\text{cur} ;\\
 			\mu_\text{cur} &\leftarrow (1-\gamma) \mu_\Theta + \gamma \hat{\mu}_\text{cur}; \\
 			\Sigma_\text{cur} &\leftarrow (1-\gamma^2) \Sigma_\Theta + \gamma^2 \hat{\Sigma}_\text{cur};
 		\end{align*} \label{Gaussian Process Update Procedure in Bandit}

 	}

 	$\mu_{\text{start}},\Sigma_{\text{start}} \leftarrow \mu_{\text{cur}},\Sigma_{\text{cur}}$ \tcp*{Set start of next day posterior as current posterior}

 }
 \caption{HeartSteps Full Bandit Algorithm}
 \label{HeartSteps Full Bandit Algorithm}
\end{algorithm}

\clearpage


\newpage

\section{Training Bandit Algorithm Tuning Parameters}

For all variants of Thompson Sampling algorithms we utilize, we tune parameters following one of two protocols.  In the first, we solely minimize the $MUER$, and in the second, we minimize the $MUER$ subject to the standard deviation of $MUER$.

Recall that there were the following parameters when using all features of our full Bandit algorithm.

\begin{itemize}
	\item Gaussian Prior parameters $(\gamma, \mu_\Theta, \Sigma_\Theta)$
	\item Feedback controller parameters $(\lambda, N_c, T_c)$, where $N_c$ is the desired dosage (number of $1$ actions) over the past $T_c$ decision times, and $\lambda$ controls the strength of the feedback controller
	\item $\sigma^2$, an estimate of the reward noise variance
	\item Probability clipping $(\pi_{\text{min}}, \pi_{\text{max}})$ 
	\item Baseline Features $f_1: \mathcal{S} \to \mathbb{R}^{p_1}, f_2:\mathcal{S} \to \mathbb{R}^{p_2}$.  We set these to either the {\it Full set} or {\it Small set}
\end{itemize}

\noindent We set some values of parameters for which we have a good sense of prior as the following:

\begin{enumerate}
	\item Set $\pi_{\text{min}} = 0.1, \pi_{\text{max}} = 0.8$, set by domain science of the minimum engagement and maximum burden that users can handle.
	\item Set $\sigma^2$ to $\hat{\sigma^2}$, the empirical residual (noise) variance.
	\item Depending on variant of Bandit, set $f_1, f_2$ to either the {\it Full model}, which are the identity mappings, or the {\it Small model}, defined in \ref{Small Set Features}.
	\item Set $\mu_\Theta = \boldsymbol{0}$ as the $0$ vector in Train batches; this is motivated by the standardization of the contexts. In Test batches, set to True Training parameters.
\end{enumerate}

We aim to tune the remaining parameters by cycling through each parameter, conducting a parameter sweep, and continuing for $r_{\text{cycles}} = 4$ cycles.  We control the following parameters, where Optimization Param is the exact parameter we optimize, and Tuning Param is the corresponding parameter from the Bandit Model that it affects.  We set several seed values according to the Starting Value based on initial random search optimization, and test within the Range.

  \begin{table}[h!]
 \caption{Parameters for Optimization}
 \label{Parameter Optimization Table}
 \centering \begin{tabular*}
{0.987\textwidth}
{|p{0.2\textwidth}|p{0.1\textwidth}|p{0.35\textwidth}|p{0.08\textwidth}|p{0.11\textwidth}|}
\toprule
Optimization Param & Tuning Param & Description & Starting Value & Tested Range \\
\midrule
$\mathtt{N\_c\_mult}$ & $N_c$ & Multiplier on $T_c$ to give $N_c$ & $0.5$ & $[0.05,0.9]$ \\
$\mathtt{T\_c}$ & $T_c$ &  & $5$ & $[3,70]$ \\
$\mathtt{sig2\_mult}$ & $\sigma^2$ & Multiplier on empirical residual variance to give $\sigma^2$ & $1$ & $[0.1,2.5]$ \\
$\mathtt{gamma}$ & $\gamma$ &  & $0.9$ & $[0,1]$ \\
$\mathtt{lamb}$ & $\lambda$ & & $1.$ & $[0.1,10]$ \\
$\mathtt{prior\_cov\_mult}$ & $\Sigma_\Theta$ & Multiplier on $\mathbb{I}$ to give $\Sigma_\Theta$ & $0.5$ & $[0.1,3]$
\\\bottomrule
\end{tabular*}
  \end{table}

The order of the parameters listed in Table \ref{Parameter Optimization Table} are from most to least impactful from initial random search optimization.  Thus, we use this ordering to optimize using Algorithm \ref{Bandit Parameter Optimization, Mean MUER}, where we set $r_{\text{cycles}} = 4$ to be the number of times we cycle through optimizing the list parameters.

\begin{algorithm}[h!]
 \KwData{Starting Parameter Values, Parameter Testing Ranges}
 \KwResult{Optimal parameter values}
 Set Parameters to Starting Parameter Values\;
 \For{$1 \le n \le r_{\text{cycles}}$}{
	 \For{Parameter in Parameter List} {
	 	\For{Parameter Value in Parameter Testing Range} {
	 		Compute Mean $MUER(\text{Parameter Value})$\;
	 	}
 		Set Parameter to Parameter Value that minimizes Mean $MUER(\text{Parameter Value})$\;
	 }
 }
  Return final set of Parameters\;
 \caption{$MUER$ Minimization Optimization}
 \label{Bandit Parameter Optimization, Mean MUER}
 \end{algorithm}


 In the second type of optimization, we also consider standard deviation of $MUER$ in our minimization.  Specifically, when optimizing a parameter, we first identify the minimum standard deviation of $MUER$, then only consider values of the parameter yielding less than $\mathtt{StdCutoff} = 1.1$ times the minimum standard deviation of $MUER$. This is shown in algorithm \ref{Bandit Parameter Optimization, STD and Mean MUER}. \\


\begin{algorithm}[h!]
 \KwData{Starting Parameter Values, Parameter Testing Ranges, $\mathtt{StdCutoff}$}
 \KwResult{Optimal parameter values}
 Set Parameters to Starting Parameter Values\;
 \For{$1 \le n \le r_{\text{cycles}}$}{
	 \For{Parameter in Parameter List} {
	 	\For{Parameter Value in Parameter Testing Range} {
	 		Compute Mean, StdDev of $MUER(\text{Parameter Value})$\;
	 	}
 		Set Parameter to Parameter Value that minimizes Mean $MUER(\text{Parameter Value})$, subject to \begin{align*} &StdDev\left(MUER(\text{Parameter Value})\right) \\ <&\mathtt{StdCutoff} * \min StdDev\left(MUER(\text{Parameter Value})\right);\end{align*}
	 }
 }
  Return final set of Parameters\;
 \caption{Standard Deviation Cutoff Optimization}
 \label{Bandit Parameter Optimization, STD and Mean MUER}
 \end{algorithm}

